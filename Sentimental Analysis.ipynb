{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I followed two approaches with different order of functions for the prediction\n",
    "###  Approach: 1) Prepocessed the data, removed Stopwords, Tokenization, Count Vectorization /TF-IDF, Scaling and 3 models (Logistic Regression, Naïve Bayes ) using GridsearchCV with Macro-f1 scoring\n",
    "#### ---- I observed that the scoring was coming neared to 60% but not much greater than that\n",
    "\n",
    "###  Approach: 2) Tokenization,Removed Stopwords, Preprocessing the data, Count Vectorization /TF-IDF, Scaling  with Normalization and 3 models (Logistic Regression, Naïve Bayes ) using GridsearchCV with Macro-f1 scoring\n",
    "#### ---- I observed that the scoring improved by approximately 8% when compared with the above approach. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "import spacy\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import  SpaceTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# Set log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\14692\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@USAirways  ! THE WORST in customer service. @...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@united call wait times are over 20 minutes an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@JetBlue what's up with the random delay on fl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@AmericanAir Good morning!  Wondering why my p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@united UA 746. Pacific Rim and Date Night cut...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text  Target\n",
       "0   1  @USAirways  ! THE WORST in customer service. @...      -1\n",
       "1   2  @united call wait times are over 20 minutes an...      -1\n",
       "2   3  @JetBlue what's up with the random delay on fl...      -1\n",
       "3   4  @AmericanAir Good morning!  Wondering why my p...       0\n",
       "4   5  @united UA 746. Pacific Rim and Date Night cut...      -1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(\"C:\\\\Users\\\\14692\\\\Documents\\\\Sem -III\\\\NLP\\\\train.csv\",encoding='latin1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7322</td>\n",
       "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7323</td>\n",
       "      <td>@AmericanAir after all, the plane didnÂÃÂªt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7324</td>\n",
       "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7325</td>\n",
       "      <td>@USAirways I can legitimately say that I would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7326</td>\n",
       "      <td>@AmericanAir still no response from AA. great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  7322  @AmericanAir In car gng to DFW. Pulled over 1h...\n",
       "1  7323  @AmericanAir after all, the plane didnÂÃÂªt ...\n",
       "2  7324  @SouthwestAir can't believe how many paying cu...\n",
       "3  7325  @USAirways I can legitimately say that I would...\n",
       "4  7326  @AmericanAir still no response from AA. great ..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file=pd.read_csv(\"C:\\\\Users\\\\14692\\\\Documents\\\\Sem -III\\\\NLP\\\\test.csv\",encoding='latin1')\n",
    "upload_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    numbers           = r\"\\d*\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' ',tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, emojis[emoji])        \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' ', tweet)        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        \n",
    "        #Remove numbers\n",
    "        tweet=re.sub(numbers,\"\",tweet)\n",
    "        \n",
    "        \n",
    "            \n",
    "        processedText.append(tweet)\n",
    "        \n",
    "        \n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       @USAirways  ! THE WORST in customer service. @...\n",
       "1       @united call wait times are over 20 minutes an...\n",
       "2       @JetBlue what's up with the random delay on fl...\n",
       "3       @AmericanAir Good morning!  Wondering why my p...\n",
       "4       @united UA 746. Pacific Rim and Date Night cut...\n",
       "                              ...                        \n",
       "7315                              @AmericanAir followback\n",
       "7316    @united thanks for the help. Wish the phone re...\n",
       "7317    @usairways the. Worst. Ever. #dca #customerser...\n",
       "7318    @nrhodes85: look! Another apology. DO NOT FLY ...\n",
       "7319    @united you are by far the worst airline. 4 pl...\n",
       "Name: text, Length: 7320, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintext=train['text']\n",
    "testtext=upload_file['text']\n",
    "# traintext=(train['text']).apply(lambda x:preprocess(x))\n",
    "# traintext=pd.DataFrame(traintext, columns=['text'])\n",
    "traintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removestopwords(data):\n",
    "    words_only = re.sub(\"[^a-zA-Z]\",\" \",str(data))       \n",
    "\n",
    "    words = words_only.lower().split()     \n",
    "    stops = set(stopwords.words(\"english\"))      \n",
    "    tokenize_words = [w for w in words if not w in stops]      \n",
    "    return (\" \".join(tokenize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopwords(data):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = data.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traintext1=list(map(RemoveStopwords,traintext))\n",
    "testtext1=list(map(RemoveStopwords,testtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divided the set into 80:20 ratio train-test set\n",
    "X_train=(traintext1[0:int(len(train)*0.8)])\n",
    "y_train=train['Target'][0:int(len(train)*0.8)]\n",
    "X_test=(traintext1[int(len(train)*0.8):])\n",
    "y_test=train['Target'][int(len(train)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file, tokenizer):\n",
    "    corpus=[]\n",
    "    for sentence in file:\n",
    "        tokens=' '.join(tokenizer(sentence))\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy lemmatization needs tagger but disable the rest\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "\n",
    "def get_part_of_speech_tags(token):\n",
    "    \n",
    "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
    "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenizer(data):\n",
    "    tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text=tknzr.tokenize(data)\n",
    "    return text\n",
    "\n",
    "# def punkt_tokenizer(data):\n",
    "#     sent_tokenizer = PunktSentenceTokenizer(data)\n",
    "#     return sent_tokenizer\n",
    "    \n",
    "def whitespace_tokenizer(data):\n",
    "    return str.split(data)\n",
    "\n",
    "def potter_tokenizer(data):\n",
    "    tokenizer= potter_tokenizer()\n",
    "    return tokenizer.tokenize(data)\n",
    "\n",
    "def spacy_tokenizer(data):\n",
    "    spacy_tokens=nlp(data)\n",
    "    return [token.text for token in spacy_tokens]\n",
    "\n",
    "def spacy_lemmatizer(data):\n",
    "    spacy_tokens=nlp(data)\n",
    "    return [token.lemma_ for token in spacy_tokens]\n",
    "\n",
    "def nltk_treebank_tokenizer(data):\n",
    "    tokenizer= TreebankWordTokenizer()\n",
    "    return tokenizer.tokenize(data)\n",
    "\n",
    "def nltk_treebank_stemmer_tokenizer(data):\n",
    "    tokenizer= TreebankWordTokenizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens= tokenizer.tokenize(data)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def nltk_treebank_lemmatizer_tokenizer(data):\n",
    "    tokenizer= TreebankWordTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    tokens= tokenizer.tokenize(data)\n",
    "    return [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tokenizer(tokenizer, X_train, Y_train,X_test, Y_test, cv):\n",
    "\n",
    "\n",
    "    X_train=read_data(X_train, tokenizer)\n",
    "    X_test=read_data(X_test, tokenizer)\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "    X_train_cv = vectorizer.fit_transform(X_train)\n",
    "    X_test_cv = vectorizer.transform(X_test)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler(with_mean= False).fit(X_train_cv)\n",
    "    X_train_cv=scaler.transform(X_train_cv) #Scaling the data\n",
    "    X_test_cv=scaler.transform(X_test_cv) #Scaling the data\n",
    "\n",
    "    vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=60000,use_idf=True, smooth_idf=True)\n",
    "    X_train_tf=vectoriser.fit_transform(X_train)\n",
    "    X_test_tf=vectoriser.transform(X_test)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler(with_mean= False).fit(X_train_tf)\n",
    "    X_train_tf=scaler.transform(X_train_tf) #Scaling the data\n",
    "    X_test_tf=scaler.transform(X_test_tf) #Scaling the data\n",
    "\n",
    "    #fitting the model for Count Vectorizer\n",
    "#         logreg_cv = linear_model.LogisticRegression(C=1.0,max_iter=1000, penalty='l2')\n",
    "#         logreg_cv.fit(X_train_cv, Y_train)\n",
    "#         y_pred_cv=logreg_cv.predict(X_test_cv)\n",
    "\n",
    "    parameters={'C': [2,3,4], 'max_iter': [1000], 'n_jobs': [-1], 'penalty': ['l2']}\n",
    "    grid_cv = GridSearchCV(LogisticRegression(), param_grid=parameters, cv=cv)\n",
    "    grid_cv.fit(X_train_cv, Y_train)\n",
    "    y_pred_cv = grid_cv.predict(X_test_cv)\n",
    "    print(\"Macro f score of %s for Count Vectorizer is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_cv, average='macro')))\n",
    "\n",
    "    #fitting the model for TF-IDF\n",
    "#         logreg_tf = linear_model.LogisticRegression(C=1.0,max_iter=1000, penalty='l2')\n",
    "#         logreg_tf.fit(X_train_tf, Y_train)\n",
    "#         y_pred_tf=logreg_tf.predict(X_test_tf)\n",
    "    grid_tf = GridSearchCV(LogisticRegression(), param_grid=parameters, cv=cv)\n",
    "    grid_tf.fit(X_train_tf, Y_train)\n",
    "    y_pred_tf = grid_tf.predict(X_test_tf)\n",
    "    print(\"Macro f score %s for TF-IDF is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_tf, average='macro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro f score of whitespace_tokenizer for Count Vectorizer is 0.588467\n",
      "Macro f score whitespace_tokenizer for TF-IDF is 0.576487\n",
      "None\n",
      "Macro f score of spacy_tokenizer for Count Vectorizer is 0.587798\n",
      "Macro f score spacy_tokenizer for TF-IDF is 0.574157\n",
      "None\n",
      "Macro f score of nltk_treebank_tokenizer for Count Vectorizer is 0.589661\n",
      "Macro f score nltk_treebank_tokenizer for TF-IDF is 0.576454\n",
      "None\n",
      "Macro f score of nltk_treebank_stemmer_tokenizer for Count Vectorizer is 0.600214\n",
      "Macro f score nltk_treebank_stemmer_tokenizer for TF-IDF is 0.585200\n",
      "None\n",
      "Macro f score of nltk_treebank_lemmatizer_tokenizer for Count Vectorizer is 0.608647\n",
      "Macro f score nltk_treebank_lemmatizer_tokenizer for TF-IDF is 0.588033\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#for cv=5\n",
    "tokenizers= [whitespace_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
    "for tokenizer in tokenizers:\n",
    "    print(evaluate_tokenizer(tokenizer, X_train, y_train,X_test, y_test, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro f score of whitespace_tokenizer for Count Vectorizer is 0.588467\n",
      "Macro f score whitespace_tokenizer for TF-IDF is 0.576487\n",
      "None\n",
      "Macro f score of spacy_tokenizer for Count Vectorizer is 0.587798\n",
      "Macro f score spacy_tokenizer for TF-IDF is 0.574157\n",
      "None\n",
      "Macro f score of nltk_treebank_tokenizer for Count Vectorizer is 0.589661\n",
      "Macro f score nltk_treebank_tokenizer for TF-IDF is 0.576454\n",
      "None\n",
      "Macro f score of nltk_treebank_stemmer_tokenizer for Count Vectorizer is 0.600214\n",
      "Macro f score nltk_treebank_stemmer_tokenizer for TF-IDF is 0.585200\n",
      "None\n",
      "Macro f score of nltk_treebank_lemmatizer_tokenizer for Count Vectorizer is 0.608647\n",
      "Macro f score nltk_treebank_lemmatizer_tokenizer for TF-IDF is 0.588033\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#for cv=3\n",
    "tokenizers= [whitespace_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
    "for tokenizer in tokenizers:\n",
    "    print(evaluate_tokenizer(tokenizer, X_train, y_train,X_test, y_test,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now changing the order of execution of functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tokenizer(tokenizer, X_train, Y_train,X_test, Y_test, cv):\n",
    "\n",
    "    #Tokenization\n",
    "    X_train=read_data(X_train, tokenizer)\n",
    "    X_test=read_data(X_test, tokenizer)\n",
    "\n",
    "    #Again Remove Stopwords\n",
    "    X_train=list(map(RemoveStopwords,X_train))\n",
    "    X_test=list(map(RemoveStopwords,X_test))\n",
    "    \n",
    "    #Preprocessing\n",
    "    X_train=preprocess(X_train)\n",
    "    X_test=preprocess(X_test)\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "    X_train_cv = vectorizer.fit_transform(X_train)\n",
    "    X_test_cv = vectorizer.transform(X_test)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler(with_mean= False).fit(X_train_cv)\n",
    "    X_train_cv=scaler.transform(X_train_cv) #Scaling the data\n",
    "    X_test_cv=scaler.transform(X_test_cv) #Scaling the data\n",
    "    \n",
    "    #Normalize\n",
    "    X_train_cv = preprocessing.normalize(X_train_cv)\n",
    "    X_train_cv = preprocessing.normalize(X_train_cv)\n",
    "    \n",
    "    vectoriser = TfidfVectorizer(ngram_range=(1,1), max_features=60000,use_idf=True, smooth_idf=True)\n",
    "    X_train_tf=vectoriser.fit_transform(X_train)\n",
    "    X_test_tf=vectoriser.transform(X_test)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler(with_mean= False).fit(X_train_tf)\n",
    "    X_train_tf=scaler.transform(X_train_tf) #Scaling the data\n",
    "    X_test_tf=scaler.transform(X_test_tf) #Scaling the data\n",
    "\n",
    "    #Normalize\n",
    "    X_train_tf = preprocessing.normalize(X_train_tf)\n",
    "    X_test_tf = preprocessing.normalize(X_test_tf)\n",
    "\n",
    "    #fitting the models for Count Vectorizer        \n",
    "    parameters={'C': [1,2,3,4], 'max_iter': [3000], 'warm_start': [True],'n_jobs': [-1], 'penalty': ['l2']}\n",
    "    grid_cv = GridSearchCV(LogisticRegression(), param_grid=parameters, cv=cv)\n",
    "    grid_cv.fit(X_train_cv, Y_train)\n",
    "    y_pred_cv = grid_cv.predict(X_test_cv)\n",
    "    print(\" Logistic Macro-f score of %s for Count Vectorizer is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_cv, average='macro')))\n",
    "\n",
    "    parameters={'priors':[None], 'var_smoothing':[1e-09]}\n",
    "    grid_cv = GridSearchCV(GaussianNB(), param_grid=parameters, cv=cv)\n",
    "    grid_cv.fit(X_train_cv.toarray(), Y_train)\n",
    "    y_pred_cv = grid_cv.predict(X_test_cv.toarray())\n",
    "    print(\"Naive Bayes Macro-f score of %s for Count Vectorizer is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_cv, average='macro')))\n",
    "\n",
    "    parameters={'max_depth':[2,4,5],'n_estimators':[200,400], 'learning_rate':[1]}\n",
    "    grid_cv = GridSearchCV(XGBClassifier(), param_grid=parameters, cv=cv)\n",
    "    grid_cv.fit(X_train_cv, Y_train)\n",
    "    y_pred_cv = grid_cv.predict(X_test_cv)\n",
    "    print(\"XGBoost Macro-f score of %s for Count Vectorizer is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_cv, average='macro')))\n",
    "\n",
    "    ########################################################################################\n",
    "    #fitting the models for TF-IDF\n",
    "    parameters={'C': [1,2,3,4], 'max_iter': [3000], 'warm_start': [True], 'n_jobs': [-1], 'penalty': ['l2']}\n",
    "    grid_tf = GridSearchCV(LogisticRegression(), param_grid=parameters, cv=cv)\n",
    "    grid_tf.fit(X_train_tf, Y_train)\n",
    "    y_pred_tf = grid_tf.predict(X_test_tf)\n",
    "    print(\"Logistic Macro-f score %s for TF-IDF is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_tf, average='macro')))\n",
    "\n",
    "    parameters={'priors':[None], 'var_smoothing':[1e-09]}\n",
    "    grid_tf = GridSearchCV(GaussianNB(), param_grid=parameters, cv=cv)\n",
    "    grid_tf.fit(X_train_tf.toarray(), Y_train)\n",
    "    y_pred_tf = grid_tf.predict(X_test_tf.toarray())\n",
    "    print(\"Naive Bayes Macro-f score of %s for TF-IDF is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_tf, average='macro')))\n",
    "\n",
    "    parameters={'max_depth':[2,4,5],'n_estimators':[200,400],'learning_rate':[1]}\n",
    "    grid_tf = GridSearchCV(XGBClassifier(), param_grid=parameters, cv=cv)\n",
    "    grid_tf.fit(X_train_tf, Y_train)\n",
    "    y_pred_tf = grid_tf.predict(X_test_tf)\n",
    "    print(\"XGBoost Macro-f score of %s for TF-IDF is %f\" % (tokenizer.__name__, f1_score(Y_test, y_pred_tf, average='macro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Macro-f score of tweet_tokenizer for Count Vectorizer is 0.643350\n",
      "Naive Bayes Macro-f score of tweet_tokenizer for Count Vectorizer is 0.554394\n",
      "XGBoost Macro-f score of tweet_tokenizer for Count Vectorizer is 0.632155\n",
      "Logistic Macro-f score tweet_tokenizer for TF-IDF is 0.603429\n",
      "Naive Bayes Macro-f score of tweet_tokenizer for TF-IDF is 0.490938\n",
      "XGBoost Macro-f score of tweet_tokenizer for TF-IDF is 0.705212\n",
      "None\n",
      " Logistic Macro-f score of whitespace_tokenizer for Count Vectorizer is 0.642393\n",
      "Naive Bayes Macro-f score of whitespace_tokenizer for Count Vectorizer is 0.558992\n",
      "XGBoost Macro-f score of whitespace_tokenizer for Count Vectorizer is 0.640975\n",
      "Logistic Macro-f score whitespace_tokenizer for TF-IDF is 0.609373\n",
      "Naive Bayes Macro-f score of whitespace_tokenizer for TF-IDF is 0.489211\n",
      "XGBoost Macro-f score of whitespace_tokenizer for TF-IDF is 0.715166\n",
      "None\n",
      " Logistic Macro-f score of spacy_tokenizer for Count Vectorizer is 0.644218\n",
      "Naive Bayes Macro-f score of spacy_tokenizer for Count Vectorizer is 0.558784\n",
      "XGBoost Macro-f score of spacy_tokenizer for Count Vectorizer is 0.626539\n",
      "Logistic Macro-f score spacy_tokenizer for TF-IDF is 0.608960\n",
      "Naive Bayes Macro-f score of spacy_tokenizer for TF-IDF is 0.490080\n",
      "XGBoost Macro-f score of spacy_tokenizer for TF-IDF is 0.712644\n",
      "None\n",
      " Logistic Macro-f score of nltk_treebank_tokenizer for Count Vectorizer is 0.647105\n",
      "Naive Bayes Macro-f score of nltk_treebank_tokenizer for Count Vectorizer is 0.541911\n",
      "XGBoost Macro-f score of nltk_treebank_tokenizer for Count Vectorizer is 0.622049\n",
      "Logistic Macro-f score nltk_treebank_tokenizer for TF-IDF is 0.616098\n",
      "Naive Bayes Macro-f score of nltk_treebank_tokenizer for TF-IDF is 0.496245\n",
      "XGBoost Macro-f score of nltk_treebank_tokenizer for TF-IDF is 0.696705\n",
      "None\n",
      " Logistic Macro-f score of nltk_treebank_stemmer_tokenizer for Count Vectorizer is 0.637001\n",
      "Naive Bayes Macro-f score of nltk_treebank_stemmer_tokenizer for Count Vectorizer is 0.532692\n",
      "XGBoost Macro-f score of nltk_treebank_stemmer_tokenizer for Count Vectorizer is 0.611564\n",
      "Logistic Macro-f score nltk_treebank_stemmer_tokenizer for TF-IDF is 0.623423\n",
      "Naive Bayes Macro-f score of nltk_treebank_stemmer_tokenizer for TF-IDF is 0.461947\n",
      "XGBoost Macro-f score of nltk_treebank_stemmer_tokenizer for TF-IDF is 0.707190\n",
      "None\n",
      " Logistic Macro-f score of nltk_treebank_lemmatizer_tokenizer for Count Vectorizer is 0.644872\n",
      "Naive Bayes Macro-f score of nltk_treebank_lemmatizer_tokenizer for Count Vectorizer is 0.554542\n",
      "XGBoost Macro-f score of nltk_treebank_lemmatizer_tokenizer for Count Vectorizer is 0.602645\n",
      "Logistic Macro-f score nltk_treebank_lemmatizer_tokenizer for TF-IDF is 0.627226\n",
      "Naive Bayes Macro-f score of nltk_treebank_lemmatizer_tokenizer for TF-IDF is 0.464692\n",
      "XGBoost Macro-f score of nltk_treebank_lemmatizer_tokenizer for TF-IDF is 0.685975\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#for cv=2\n",
    "tokenizers= [tweet_tokenizer, whitespace_tokenizer, spacy_tokenizer, nltk_treebank_tokenizer, nltk_treebank_stemmer_tokenizer, nltk_treebank_lemmatizer_tokenizer]\n",
    "for tokenizer in tokenizers:\n",
    "    print(evaluate_tokenizer(tokenizer, X_train, y_train,X_test, y_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost with whitespace_tokenizer performs the best among all the models with combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Target value of test data from the above results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tokenizer(tokenizer, X_train, Y_train,X_test, cv):\n",
    "        \n",
    "   #Tokenization\n",
    "    X_train=read_data(X_train, tokenizer)\n",
    "    X_test=read_data(X_test, tokenizer)\n",
    "    \n",
    "    #Preprocessing\n",
    "    traintext=preprocess(X_train)\n",
    "    testtext=preprocess(X_test)\n",
    "\n",
    "#     #Again Remove Stopwords\n",
    "#     X_train=list(map(RemoveStopwords,traintext))\n",
    "#     X_test=list(map(RemoveStopwords,testtext))\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=10000, analyzer=str.split, lowercase=False, strip_accents=None, binary=True)\n",
    "    X_train_cv = vectorizer.fit_transform(X_train)\n",
    "    X_test_cv = vectorizer.transform(X_test)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler(with_mean= False).fit(X_train_cv)\n",
    "    X_train_cv=scaler.transform(X_train_cv) #Scaling the data\n",
    "    X_test_cv=scaler.transform(X_test_cv) #Scaling the data\n",
    "\n",
    "    #Normalize\n",
    "    X_train_cv = preprocessing.normalize(X_train_cv)\n",
    "    X_train_cv = preprocessing.normalize(X_train_cv)        \n",
    "        \n",
    "    parameters={'C': [0.001,0.01,0.1,1,2,4], 'max_iter': [2000,3000],'n_jobs': [-1], 'penalty': ['l2','newton-cg']}\n",
    "    grid_cv = GridSearchCV(LogisticRegression(), param_grid=parameters, cv=cv, n_jobs=-1)\n",
    "    grid_cv.fit(X_train_cv, Y_train)\n",
    "    y_pred_cv = grid_cv.predict(X_test_cv)\n",
    "    return y_pred_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0, -1, ..., -1,  1, -1], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=evaluate_tokenizer(nltk_treebank_tokenizer, traintext, train['Target'],testtext, 10)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_file['target']=y_pred\n",
    "# upload_file.to_csv (r'C:\\\\Users\\\\14692\\\\Documents\\\\Sem -III\\\\NLP\\\\predicted_final.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
